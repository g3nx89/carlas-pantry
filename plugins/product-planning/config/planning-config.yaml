# Planning Configuration
# Configuration for /product-planning:plan command with ST/PAL integration

# =============================================================================
# FEATURE FLAGS
# =============================================================================
# Toggle improvements on/off for gradual rollout and rollback capability
# See PROPOSAL-planning-enhancements.md for detailed documentation

feature_flags:
  # Universal Improvements (all modes)
  s1_self_critique:
    enabled: true
    description: "Agent self-verification before submission"
    rollback: "Set false to remove self-critique from agents"
    mode_thresholds:
      rapid: { min_questions: 3, pass_threshold: 2 }
      standard: { min_questions: 5, pass_threshold: 4 }
      advanced: { min_questions: 5, pass_threshold: 4 }
      complete: { min_questions: 5, pass_threshold: 5 }

  s2_cot_prefix:
    enabled: true
    description: "Zero-Shot Chain-of-Thought reasoning prefix"
    rollback: "Set false to remove CoT sections from agents"

  a3_adaptive_depth:
    enabled: true
    description: "Risk-based research depth adjustment"
    rollback: "Set false for mode-based depth only"

  a5_post_planning_menu:
    enabled: true
    description: "Interactive menu after planning completion"
    rollback: "Set false to skip menu"

  # Advanced/Complete Mode Improvements
  s3_judge_gates:
    enabled: true
    description: "Quality gates between phases"
    rollback: "Set false to skip gates"
    modes: [advanced, complete]
    gates:
      - after_phase: 2
        name: "Research Completeness"
      - after_phase: 4
        name: "Architecture Quality"
      - after_phase: 7
        name: "Test Coverage"

  s4_adaptive_strategy:
    enabled: true
    description: "Adaptive synthesis strategy based on evaluation"
    rollback: "Set false to always synthesize"
    modes: [advanced, complete]

  a4_expert_review:
    enabled: true
    description: "Security and simplicity expert review"
    rollback: "Set false to skip Phase 6b"
    modes: [advanced, complete]

  # Complete Mode Only Improvements
  s5_tot_architecture:
    enabled: true
    description: "Hybrid Tree-of-Thoughts architecture exploration"
    rollback: "Set false to use standard MPA"
    modes: [complete]
    requires_mcp: false  # Uses internal agents
    requires: [s4_adaptive_strategy]  # ToT needs adaptive strategy for branch selection

  s6_multi_judge_debate:
    enabled: true
    description: "Multi-round debate for validation"
    rollback: "Set false to use single-round"
    modes: [complete]
    requires_mcp: true  # Requires PAL Consensus
    requires: [s3_judge_gates]  # Debates need judge infrastructure for scoring

  # Context Improvements
  a1_flow_analysis:
    enabled: true
    description: "User flow analysis in Phase 2b"
    rollback: "Set false to skip flow analysis"
    modes: [complete, advanced, standard]

  a2_learnings_researcher:
    enabled: true
    description: "Institutional knowledge integration"
    rollback: "Set false to skip learnings search"
    modes: [complete, advanced, standard]

# =============================================================================
# ADAPTIVE RESEARCH DEPTH (A3)
# =============================================================================
# Adjusts research intensity based on risk indicators in the feature spec

research_depth:
  # Keywords that indicate higher risk (case-insensitive)
  risk_keywords:
    high:
      - payment
      - payments
      - billing
      - credit card
      - authentication
      - auth
      - authorization
      - security
      - encryption
      - password
      - credentials
      - PII
      - GDPR
      - HIPAA
      - SOC2
      - compliance
      - financial
      - transaction
    medium:
      - API
      - integration
      - migration
      - database
      - schema
      - performance
      - scaling
      - concurrent
      - async
      - cache
      - session
      - state

  # Depth levels and what they mean
  levels:
    minimal:
      agents: 1
      search_scope: "direct matches only"
      analysis_depth: "surface level"
      st_templates: []
    standard:
      agents: 2
      search_scope: "related patterns"
      analysis_depth: "moderate"
      st_templates: [T4, T5]
    deep:
      agents: 3
      search_scope: "comprehensive exploration"
      analysis_depth: "thorough"
      st_templates: [T4, T5, T6]

  # Mode × Risk interaction matrix
  # Rows: analysis mode, Columns: risk level
  depth_matrix:
    rapid:
      low: minimal
      medium: minimal
      high: standard
    standard:
      low: minimal
      medium: standard
      high: standard
    advanced:
      low: standard
      medium: standard
      high: deep
    complete:
      low: standard
      medium: deep
      high: deep

# =============================================================================
# INSTITUTIONAL KNOWLEDGE (A2)
# =============================================================================
# Configuration for learnings-researcher agent and knowledge base integration

institutional_knowledge:
  # Expected locations for knowledge base
  paths:
    solutions: "docs/solutions/"
    critical_patterns: "docs/critical-patterns.md"
    anti_patterns: "docs/anti-patterns.md"  # Optional

  # Schema for solution records
  schema_template: "$CLAUDE_PLUGIN_ROOT/templates/learnings-schema.yaml"

  # Relevance scoring criteria
  relevance:
    high:
      criteria: "Same domain + same technology + similar constraints"
      action: "Apply directly to current feature"
    medium:
      criteria: "Same domain OR same technology"
      action: "Consider for adaptation"
    low:
      criteria: "Related but different context"
      action: "Reference only"

  # Behavior when knowledge base doesn't exist
  graceful_degradation:
    log_message: "No institutional knowledge base found"
    suggest_creation: true
    continue_without: true

# =============================================================================
# ANALYSIS MODES
# =============================================================================
# Hierarchical modes with graceful degradation when MCP tools unavailable

analysis_modes:
  complete:
    description: "Full analysis with PAL ThinkDeep + Sequential Thinking + Consensus + Full Test Plan"
    mpa_agents: 6  # code-explorer, software-architect, tech-lead + 3 QA agents (MPA)
    pal_thinkdeep_total_calls: 9  # 3 perspectives × 3 models
    sequential_thinking_enabled: true
    pal_consensus_enabled: true  # Both plan validation AND test coverage validation
    requires_mcp: true
    estimated_cost: "$0.80-1.50"  # Includes 9-phase workflow with test planning

  advanced:
    description: "MPA + ThinkDeep (reduced perspectives) + Test Plan"
    mpa_agents: 5  # code-explorer, software-architect, tech-lead + 2 QA agents
    pal_thinkdeep_total_calls: 6  # 2 perspectives × 3 models
    sequential_thinking_enabled: false
    pal_consensus_enabled: true  # Test coverage validation only
    requires_mcp: true
    estimated_cost: "$0.45-0.75"

  standard:
    description: "MPA only (no PAL/ST) + Basic Test Plan"
    mpa_agents: 4  # code-explorer, software-architect, tech-lead + qa-strategist
    pal_thinkdeep_total_calls: 0
    sequential_thinking_enabled: false
    pal_consensus_enabled: false
    requires_mcp: false
    estimated_cost: "$0.15-0.30"

  rapid:
    description: "Single agent fast path + Minimal Test Plan"
    mpa_agents: 2  # software-architect + qa-strategist
    pal_thinkdeep_total_calls: 0
    sequential_thinking_enabled: false
    pal_consensus_enabled: false
    requires_mcp: false
    estimated_cost: "$0.05-0.12"

# Note on model names: The model identifiers (gpt-5.2, gemini-3-pro-preview, grok-4) are
# configurable placeholders. Update these to match your available PAL/MCP model configurations.

# =============================================================================
# MODE AUTO-SUGGESTION
# =============================================================================
# Heuristics for suggesting analysis mode based on feature characteristics

mode_suggestion:
  enabled: true
  description: "Automatic mode suggestion based on feature characteristics"

  # Evaluation order: first matching rule wins
  rules:
    - name: "complex_feature"
      conditions:
        spec_word_count: ">= 2000"
        OR:
          high_risk_keywords: ">= 3"
      suggest: complete
      rationale: "Large spec or multiple high-risk areas require full analysis"

    - name: "risky_feature"
      conditions:
        high_risk_keywords: ">= 2"
        OR:
          estimated_files: ">= 15"
      suggest: advanced
      rationale: "Significant risk or large scope benefits from PAL insights"

    - name: "moderate_feature"
      conditions:
        spec_word_count: ">= 500"
        OR:
          estimated_files: ">= 5"
      suggest: standard
      rationale: "Moderate complexity benefits from MPA without external models"

    - name: "simple_feature"
      conditions:
        default: true
      suggest: rapid
      rationale: "Small features can use fast path"

  # Keyword patterns for risk detection (reuses research_depth.risk_keywords)
  risk_detection:
    use_config: "research_depth.risk_keywords"

  # Output format for suggestion
  output_template: |
    Detected: {high_risk_count} high-risk keywords ({keywords_sample})
    Estimated: {file_count} files affected
    Recommended: {mode} mode (~{cost_estimate})

# Graceful degradation rules
degradation:
  if_pal_unavailable:
    complete: standard
    advanced: standard
  if_st_unavailable:
    complete: advanced  # Keeps PAL but loses ST
    use_internal_reasoning: true
    mark_degraded: true

# =============================================================================
# SEQUENTIAL THINKING
# =============================================================================

sequential_thinking:
  # Template groups loaded from templates/sequential-thinking-templates.md
  template_groups:
    problem_decomposition: [T1, T2, T3]
    codebase_analysis: [T4, T5, T6]
    architecture_design: [T7, T8, T9, T10]
    risk_assessment: [T11, T12, T13]
    plan_validation: [T14, T15, T16]

  # When to use each group
  phase_mapping:
    setup: null  # No ST needed
    research: [codebase_analysis]
    clarification: [problem_decomposition]
    architecture: [architecture_design, risk_assessment]
    validation: [plan_validation]

  # Checkpoint after each group
  checkpoint_after_group: true

# =============================================================================
# PAL THINKDEEP
# =============================================================================

pal:
  thinkdeep:
    # Models to use (in order of preference)
    # NOTE: These are configurable placeholders. Update to match your PAL MCP server's
    # available models. Common alternatives: claude-3-opus, gpt-4-turbo, gemini-pro
    models:
      - gpt-5.2
      - gemini-3-pro-preview
      - openrouter/x-ai/grok-4  # For diversity

    # Architecture-focused perspectives
    perspectives:
      performance:
        focus: scalability, latency, resource_efficiency, bottlenecks
        prompt_template: |
          Analyze this architecture plan from a PERFORMANCE perspective.

          MY CURRENT ANALYSIS:
          {current_analysis}

          EXTEND MY ANALYSIS - Focus on:
          1. Scalability bottlenecks and horizontal/vertical scaling implications
          2. Latency-sensitive paths and potential optimizations
          3. Resource efficiency (memory, CPU, I/O patterns)
          4. Caching strategies and data locality
          5. Async patterns and parallelization opportunities
        run_on_all_models: true

      maintainability:
        focus: code_quality, extensibility, technical_debt, modularity
        prompt_template: |
          Analyze this architecture plan from a MAINTAINABILITY perspective.

          MY CURRENT ANALYSIS:
          {current_analysis}

          EXTEND MY ANALYSIS - Focus on:
          1. Coupling and cohesion of proposed components
          2. Extensibility for future requirements
          3. Technical debt introduced vs. avoided
          4. Testing strategy and testability
          5. Code patterns that aid or hinder maintainability
        run_on_all_models: true

      security:
        focus: threat_modeling, compliance, vulnerabilities, data_protection
        prompt_template: |
          Analyze this architecture plan from a SECURITY perspective.

          MY CURRENT ANALYSIS:
          {current_analysis}

          EXTEND MY ANALYSIS - Focus on:
          1. Threat modeling (STRIDE categories)
          2. Authentication and authorization patterns
          3. Data protection (at rest, in transit)
          4. Input validation and injection prevention
          5. Compliance considerations (GDPR, SOC2, etc.)
        run_on_all_models: true

    # Mode variations
    mode_matrix:
      complete:
        perspectives: [performance, maintainability, security]
        models_per_perspective: 3
        total_calls: 9
      advanced:
        perspectives: [performance, security]  # Skip maintainability
        models_per_perspective: 3
        total_calls: 6

    # Problem context template
    problem_context_template: |
      IMPORTANT: This is ARCHITECTURE PLANNING, not code review.
      We are designing the implementation approach for a feature.
      Please analyze from an architecture perspective.

      FEATURE SUMMARY:
      {feature_summary}

      CODEBASE CONTEXT:
      {codebase_patterns}

    # Output aggregation
    output:
      file: "{FEATURE_DIR}/analysis/thinkdeep-insights.md"
      include_convergent: true  # Where all models agree
      include_divergent: true   # Where models disagree → FLAG for decisions
      priority_from_convergence: true  # Agreement = higher priority

  # =============================================================================
  # PAL CONSENSUS
  # =============================================================================

  consensus:
    min_models: 2  # Minimum for valid consensus

    # Models with stances
    models:
      - model: gemini-3-pro-preview
        stance: neutral
        stance_prompt: "Evaluate objectively"
      - model: gpt-5.2
        stance: for
        stance_prompt: "Advocate for this plan's strengths"
      - model: openrouter/x-ai/grok-4
        stance: against
        stance_prompt: "Challenge this plan. Find weaknesses and risks."

    # Scoring dimensions (20 points total, 4 per dimension)
    dimensions:
      problem_understanding:
        weight: 0.20
        criteria: "Clear problem statement, root cause identified, scope defined"
      architecture_quality:
        weight: 0.25
        criteria: "Sound design principles, appropriate patterns, good abstractions"
      risk_mitigation:
        weight: 0.20
        criteria: "Risks identified, mitigations planned, fallback strategies"
      implementation_clarity:
        weight: 0.20
        criteria: "Clear steps, dependencies mapped, acceptance criteria defined"
      feasibility:
        weight: 0.15
        criteria: "Realistic scope, resource considerations, timeline alignment"

    # Thresholds with explicit comparison semantics (per CLAUDE.md guidelines)
    # GREEN: score >= 16 → proceed to implementation
    # YELLOW: score >= 12 AND score < 16 → proceed with documented risks
    # RED: score < 12 → requires revision (boundary value: 12)
    thresholds:
      green: 16       # score >= 16: proceed to implementation
      yellow: 12      # score >= 12 AND < 16: proceed with documented risks
      red: 12         # score < 12: requires revision (boundary is 12)

    # Output
    output:
      file: "{FEATURE_DIR}/analysis/consensus-validation.md"
      include_per_model_scores: true
      include_debate_summary: true

# =============================================================================
# MPA (MULTI-PERSPECTIVE ANALYSIS)
# =============================================================================

mpa:
  agents:
    # Phase 2-4 Agents
    code_explorer:
      file: "agents/code-explorer.md"
      focus: "Codebase patterns, similar features, integration points"
      output: "{FEATURE_DIR}/analysis/codebase-analysis.md"

    software_architect:
      file: "agents/software-architect.md"
      focus: "Architecture options, design trade-offs, component design"
      output: "{FEATURE_DIR}/analysis/architecture-options.md"

    tech_lead:
      file: "agents/tech-lead.md"
      focus: "Task breakdown, dependencies, complexity, risks"
      output: "{FEATURE_DIR}/analysis/task-breakdown.md"

    # Phase 7 QA Agents (MPA for test planning)
    qa_strategist:
      file: "agents/qa-strategist.md"
      focus: "V-Model test strategy, UAT scripts, coverage matrix"
      output: "{FEATURE_DIR}/analysis/test-strategy-general.md"
      modes: [complete, advanced, standard, rapid]

    qa_security:
      file: "agents/qa-security.md"
      focus: "Security testing, STRIDE analysis, authentication/authorization"
      output: "{FEATURE_DIR}/analysis/test-strategy-security.md"
      modes: [complete, advanced]  # Only in higher modes

    qa_performance:
      file: "agents/qa-performance.md"
      focus: "Performance testing, load/stress tests, latency requirements"
      output: "{FEATURE_DIR}/analysis/test-strategy-performance.md"
      modes: [complete, advanced]  # Only in higher modes

  synthesis:
    merge_strategy: "convergent_prioritization"
    deduplication_threshold: 0.80
    priority_weights:
      impact: 0.35
      risk: 0.30
      complexity: 0.20
      dependencies: 0.15

# =============================================================================
# STATE MANAGEMENT
# =============================================================================

state:
  file: "{FEATURE_DIR}/.planning-state.local.md"
  format: yaml_frontmatter

  checkpoints:
    - SETUP
    - RESEARCH
    - CLARIFICATION
    - ARCHITECTURE
    - THINKDEEP
    - VALIDATION
    - TEST_STRATEGY          # Phase 7: V-Model test planning
    - TEST_COVERAGE_VALIDATION  # Phase 8: Coverage validation
    - COMPLETION

  # User decisions are immutable once saved
  immutable_fields:
    - user_decisions
    - approved_architecture
    - approved_test_strategy  # Test strategy locked after Phase 8

  # Version tracking for drift detection
  version_fields:
    - spec_version
    - plan_version
    - design_version
    - test_plan_version  # Track test plan version for updates

  # Lock to prevent concurrent execution
  lock:
    file: "{FEATURE_DIR}/.planning.lock"
    timeout_minutes: 60

# =============================================================================
# LIMITS AND GUARDS
# =============================================================================

limits:
  max_clarifying_questions: 15
  max_architecture_options: 4
  max_tasks_per_story: 8
  max_rounds: 10  # Circuit breaker

guards:
  require_spec_exists: true
  require_constitution_exists: true
  block_on_gate_failure: true

# =============================================================================
# TEST PLANNING (V-MODEL)
# =============================================================================

test_planning:
  # State management for test planning skill
  state:
    file: "{FEATURE_DIR}/.test-planning-state.local.md"
    format: yaml_frontmatter
    checkpoints:
      - T1_PREREQUISITES
      - T2_RISK_ANALYSIS
      - T3_TEST_LEVEL_PLANNING
      - T4_UAT_GENERATION
      - T5_COVERAGE_VALIDATION
      - T6_OUTPUT_GENERATION

  # Sequential Thinking templates for test planning
  sequential_thinking:
    risk_analysis:
      - T-RISK-1  # Failure mode identification
      - T-RISK-2  # Risk prioritization
      - T-RISK-3  # Risk to test mapping

  # Test level definitions (V-Model alignment)
  test_levels:
    unit:
      loop: inner
      trigger: every_commit
      executor: ci_pipeline
      pattern: tdd
      scope: single_function_or_class
      mocks: all_external_dependencies

    integration:
      loop: inner
      trigger: every_pr
      executor: ci_pipeline
      pattern: post_implementation
      scope: component_interaction
      mocks: external_services_only

    e2e:
      loop: outer
      trigger: pre_merge
      executor: qa_automation
      pattern: scenario_based
      scope: complete_user_flow
      evidence_required: true

    uat:
      loop: outer
      trigger: pre_release
      executor: product_owner
      pattern: given_when_then
      scope: user_story_validation
      sign_off_required: true

    exploratory:
      loop: outer
      trigger: pre_release
      executor: qa_engineer
      pattern: charter_based
      scope: edge_cases_and_chaos
      time_boxed: true

  # Coverage validation settings
  coverage:
    # PAL Consensus for test coverage validation
    consensus:
      dimensions:
        ac_coverage:
          weight: 0.25
          criteria: "All acceptance criteria mapped to tests"
        risk_coverage:
          weight: 0.25
          criteria: "All Critical/High risks have test coverage"
        uat_completeness:
          weight: 0.20
          criteria: "UAT scripts are clear and executable by non-technical users"
        test_independence:
          weight: 0.15
          criteria: "Tests can run in isolation without dependencies"
        maintainability:
          weight: 0.15
          criteria: "Tests are not brittle or over-specified"

      # Thresholds with explicit comparison semantics (per CLAUDE.md guidelines)
      # GREEN: score >= 80% → proceed with TDD
      # YELLOW: score >= 65% AND score < 80% → proceed with documented gaps
      # RED: score < 65% → add more tests (boundary value: 65)
      thresholds:
        green: 80       # score >= 80: proceed with TDD
        yellow: 65      # score >= 65 AND < 80: proceed with documented gaps
        red: 65         # score < 65: add more tests (boundary is 65)

    # Required coverage targets
    targets:
      acceptance_criteria: 100  # Every AC must have a test
      critical_risks: 100       # Every Critical risk must have coverage
      high_risks: 100           # Every High risk must have coverage
      user_stories: 100         # Every story must have UAT

  # UAT configuration
  uat:
    format: given_when_then
    evidence:
      screenshots_required: true
      sign_off_required: true
    templates:
      script: "templates/uat-script-template.md"

  # MPA agent for test planning
  agents:
    qa_strategist:
      file: "agents/qa-strategist.md"
      focus: "Risk analysis, test level planning, UAT generation"
      output: "{FEATURE_DIR}/test-plan.md"

  # Output artifact structure
  output:
    directories:
      - "{FEATURE_DIR}/test-cases/unit/"
      - "{FEATURE_DIR}/test-cases/integration/"
      - "{FEATURE_DIR}/test-cases/e2e/"
      - "{FEATURE_DIR}/test-cases/uat/"

    artifacts:
      test_plan: "{FEATURE_DIR}/test-plan.md"
      coverage_validation: "{FEATURE_DIR}/analysis/test-coverage-validation.md"

  # Test execution order (V-Model phases)
  execution_order:
    - phase: pre_implementation
      tests: [unit]
      pattern: tdd_red
      description: "Write failing unit tests before code"

    - phase: during_implementation
      tests: [unit]
      pattern: tdd_green_refactor
      description: "Implement to pass tests, then refactor"

    - phase: post_implementation
      tests: [integration]
      pattern: verify_boundaries
      description: "Test component interactions"

    - phase: pre_merge
      tests: [e2e]
      pattern: critical_paths
      description: "Validate complete user flows"

    - phase: pre_release
      tests: [uat, exploratory]
      pattern: stakeholder_validation
      description: "Business acceptance and edge case discovery"

# =============================================================================
# OUTPUT ARTIFACTS
# =============================================================================

artifacts:
  required:
    - "{FEATURE_DIR}/plan.md"
    - "{FEATURE_DIR}/design.md"

  optional:
    - "{FEATURE_DIR}/research.md"
    - "{FEATURE_DIR}/data-model.md"
    - "{FEATURE_DIR}/contract.md"
    - "{FEATURE_DIR}/analysis/thinkdeep-insights.md"
    - "{FEATURE_DIR}/analysis/consensus-validation.md"
    - "{FEATURE_DIR}/test-plan.md"
    - "{FEATURE_DIR}/analysis/test-coverage-validation.md"

# =============================================================================
# BLESSED CONFIGURATION PROFILES
# =============================================================================
# Tested flag combinations for each mode. Use these for CI validation.
# Each profile represents a validated configuration that has been tested end-to-end.

blessed_profiles:
  rapid_default:
    description: "Minimal analysis for prototypes and quick iterations"
    mode: rapid
    flags:
      s1_self_critique: true
      s2_cot_prefix: true
      a3_adaptive_depth: false
      a5_post_planning_menu: true
      # All other flags: false (not applicable to rapid mode)
    expected_cost: "$0.05-0.12"
    use_case: "Small features, quick prototypes, low-risk changes"

  standard_default:
    description: "Typical feature development with MPA"
    mode: standard
    flags:
      s1_self_critique: true
      s2_cot_prefix: true
      a3_adaptive_depth: true
      a5_post_planning_menu: true
      a1_flow_analysis: true
      a2_learnings_researcher: true
      # Advanced/Complete flags: false
    expected_cost: "$0.15-0.30"
    use_case: "Normal features, moderate complexity, established patterns"

  advanced_default:
    description: "Important features requiring PAL insights"
    mode: advanced
    flags:
      # All universal flags: true
      s1_self_critique: true
      s2_cot_prefix: true
      a3_adaptive_depth: true
      a5_post_planning_menu: true
      a1_flow_analysis: true
      a2_learnings_researcher: true
      # All advanced flags: true
      s3_judge_gates: true
      s4_adaptive_strategy: true
      a4_expert_review: true
      # Complete-only flags: false
    expected_cost: "$0.45-0.75"
    use_case: "Important features, risky changes, cross-cutting concerns"

  complete_default:
    description: "Critical features requiring full analysis"
    mode: complete
    flags:
      # All flags: true
      s1_self_critique: true
      s2_cot_prefix: true
      a3_adaptive_depth: true
      a5_post_planning_menu: true
      a1_flow_analysis: true
      a2_learnings_researcher: true
      s3_judge_gates: true
      s4_adaptive_strategy: true
      a4_expert_review: true
      s5_tot_architecture: true
      s6_multi_judge_debate: true
    expected_cost: "$0.80-1.50"
    use_case: "Critical features, security-sensitive, high business impact"

  # Validation helper for CI
  validation:
    check_requires_dependencies: true
    check_mode_availability: true
    check_mcp_requirements: true
